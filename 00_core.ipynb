{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core utilities module\n",
    "\n",
    "> This module implements core modules for the exercies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import nltk\n",
    "#from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from multiprocessing import  Pool\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_sentiment_dataset(path):\n",
    "    '''\n",
    "    Reads the sentiment140 dataset csv file. The dataset can be downloaded from Kaggle\n",
    "    '''\n",
    "    return pd.read_csv(path,engine='python', header=None ,names=['target', 'ids', 'date','flag','user','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def subsample_dataset(df,no_lines=5000):\n",
    "    \n",
    "    \"Subsamples the dataset for faster processing\"\n",
    "    df_neg = df[df.target == 0]\n",
    "    df_pos = df[df.target == 4]\n",
    "\n",
    "    df_pos = df_pos[:no_lines]\n",
    "    df_neg = df_neg[:no_lines]\n",
    "\n",
    "    return pd.concat([df_pos,df_neg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def preprocess_text(text):\n",
    "    '''\n",
    "    Preprocess tweets to remove https links and mentions\n",
    "    Remove punctioation\n",
    "    Stemmize the words\n",
    "    '''\n",
    "    \n",
    "    # Make lowercase\n",
    "    text = text.lower()\n",
    "    #print(text)\n",
    "\n",
    "    # Remove mentions and http links\n",
    "    text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", text)\n",
    "\n",
    "    # Remove punctionation\n",
    "    text = ''.join(char for char in text if char not in string.punctuation)\n",
    "\n",
    "    # Seperate text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "\n",
    "    # Stemmize\n",
    "    porter = nltk.stem.porter.PorterStemmer()\n",
    "    stemmed = [porter.stem(word) for word in words]\n",
    "    \n",
    "    \n",
    "    #print(words)\n",
    "\n",
    "    #print(stemmed)\n",
    "\n",
    "    return stemmed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def parallelize(data, func, num_of_processes=8):\n",
    "    '''\n",
    "    Parallelize the dataframe processing\n",
    "    '''\n",
    "    \n",
    "    data_split = np.array_split(data, num_of_processes)\n",
    "    pool = Pool(num_of_processes)\n",
    "    data = pool.map(func, data_split)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def preprocess_df_text(df):\n",
    "    '''\n",
    "    Preprocesses the text field of dataframe\n",
    "    '''\n",
    "    data = []\n",
    "    for index,item in df.iterrows():\n",
    "        data.extend(preprocess_text(item.text))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def calculate_word_attributes(df):\n",
    "    '''\n",
    "    Calculates the word attributes in a dataframe such as \n",
    "    total words, unique words and word frequency dictionary\n",
    "    '''\n",
    "\n",
    "    word_lists=parallelize(df,preprocess_df)\n",
    "    all_words = []\n",
    "    for item in word_lists:\n",
    "        all_words.extend(item)\n",
    "    \n",
    "    unique_all_words = list(set(all_words))\n",
    "    \n",
    "    word_freq_dict = {}\n",
    "    for item in unique_all_words:\n",
    "        word_freq_dict[item] = all_words.count(item)\n",
    "    \n",
    "    return all_words,unique_all_words,word_freq_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('hurcan': conda)",
   "language": "python",
   "name": "python37364bithurcanconda4a28e7dd692e4e1a90f4e77c54a19139"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
